This post is about my first project at Metis, a group project with the goal to learn how to analyze data with Python libraries. 

A fictional client, WomenTechWomenYes, asked which NYC subway stations were ideal locations to place solicitors for a tech-themed fundraising gala. Fortunately, MTA’s website provides data recording foot traffic for every turnstile that exists in NYC over several years. Keeping in mind that WTWY has specific target demographics, we used MTA’s foot traffic data to find the busiest subway stations in the appropriate neighborhoods.

We ended up spending the most time on interpreting and cleaning up the data. Questions that did not have obvious answers:
* Were records for each time period unique?
* Were station ids unique, or could there be more than 1 station with the same id?
* Could control areas overlap and a single turnstile could belong to more than 1?
* Could a single person passing through a turnstile have been counted multiple times somehow? 
* How could we identify unique passengers? A person’s typical journey would go from Station A to Station B in the morning, then Station B to Station A in the evening. These stations would show two traffic peaks, but it wouldn’t be helpful to advertise to the same people twice.

Then there was data that was just plain bad. Some rows recorded too many people, or negative people. Without more time and resources to investigate further, we chalked these up to turnstile error and excluded them from our analysis. Traffic below zero was obviously wrong. The upper bound was more tricky to determine, so we guessed that more than 1 person passing through a turnstile per second was probably unrealistic. Under these criteria 7-17% of the data had to be thrown out.  

We were able to produce a program that, for a given list of stations and time intervals, calculates hourly traffic per day averaged over that time. Represented visually, it was clear which locations had the potential to expose the most people to WTWY’s broadcasts.

Here are our parameters for the demo:
* 5 stations: CANAL ST, 28 ST, 14 ST, 33 ST, 23 ST. These were selected after we mapped “points of interest” (organizations such as Google, Metis, Galvanize, Cornell University, STEM schools) and found these were the stations closest to the biggest clusters.
* Time interval: We were interested in March-May, and selected years 2016-2018.

We hypothesized that traffic would follow different patterns on weekdays versus weekends, so we separated that data and graphed them separately. 

![station_totals](https://raw.githubusercontent.com/jhudiono/jhudiono.github.io/master/images/station_totals.png)

We also graphed the distribution of travelers per station entrance (control area). Here's the one for Canal Street station.

![canal_entrances](https://raw.githubusercontent.com/jhudiono/jhudiono.github.io/master/images/canal_entrances.png)

The more I thought about this problem, the more automation I wanted to throw at it. We manually selected stations to analyze based on eyeballing a rough map with points of interests marked. This was ok for our prototype, but I wasn’t confident that the result was informed enough.

Also, a very glaring oversight brought up by the other group: we did not measure the number of employees for each point. For example, a cluster of 20 startups might have a total of 1000 tech workers, but a big company like IBM has tens of thousands, even if it’s the only point of interest in the area!

To go even further, maybe each station’s “desirability score” could be calculated as a function of several factors:
* Traffic
* Sum of relevance of each "Point of Interest" within a certain distance. This could be calculated from:
    * Number of employees
    * Distance from station
    * Prestige
    * History of donating to women-oriented causes
* Area affluence
* Area diversity/minority representation

Another possible constraint to add: the client’s resources. The problem didn’t mention any of this, but in reality I would expect the client to have a limited X number of solicitors with limited schedules. We can’t recommend Z Station from 6-10PM on Saturday if none of the solicitors can show up at that time. 

Then each solicitor might have a realistic limit on how many people they can interact with. Let’s say a team can reach X% of Y people. Is it worth it to try to reach 100% by returning to the same station or scheduling more than 1 team?

For example, using random numbers: Station A has 11,000 people, the next largest Station B has 4,000 people. Each team can talk to 5,000 people. Limiting one team per station results in 5,000+4,000=9,000. But putting both teams at Station A results in 5,000+5,000=10,000.

Ultimately I’d like to make our solution more robust by including more variables that can be adjusted as necessary by the client. We had to make certain assumptions based on intelligent guesses. If it turns out we were wrong, or the client disagreed, then we can quickly update our solution. Additionally, the client can reuse our software for different dates, locations, and target demographics. 

